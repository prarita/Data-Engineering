{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b2644a",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "### Group 01    \n",
    "    \n",
    "### Group Members:\n",
    "\n",
    "| Name           | BITSID         | Weightage |\n",
    "|:---------------|:---------------|:----------|\n",
    "| Ajay Saxena    | 2021sc04160    |**100%**   |\n",
    "| MOVVA MANASWI  | 2021sc04180    |**100%**   |\n",
    "| PRARITA ARORA  | 2021sc04049    |**100%**   |\n",
    "| ADITI          | 2021sc04050    |**100%**   |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad943105",
   "metadata": {},
   "source": [
    "\n",
    "#### Question no. 01. Read the data in yellow_tripdata_2020-06.csv file into a dataframe created in spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874d27b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|    01-06-2020 00:31|     01-06-2020 00:49|              1|          3.6|         140|          68|           1|       15.5|  3.0|    0.5|       4.0|         0.0|                  0.3|        23.3|\n",
      "|    01-06-2020 00:42|     01-06-2020 01:04|              1|          5.6|          79|         226|           1|       19.5|  3.0|    0.5|       2.0|         0.0|                  0.3|        25.3|\n",
      "|    01-06-2020 00:39|     01-06-2020 00:49|              1|          2.3|         238|         116|           2|       10.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        11.3|\n",
      "|    01-06-2020 00:56|     01-06-2020 01:11|              1|          5.3|         141|         116|           2|       17.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        21.3|\n",
      "|    01-06-2020 00:16|     01-06-2020 00:29|              1|          4.4|         186|          75|           1|       14.5|  3.0|    0.5|      3.65|         0.0|                  0.3|       21.95|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"user/datat/input/yellow_tripdata_2020-06.csv\"\n",
    "\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame to ensure it's loaded correctly\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913f83f",
   "metadata": {},
   "source": [
    "### Step 5: Explore the Data\n",
    "Now we can use the PySpark DataFrame API to work with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d59161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "664747e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549760"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a388de1",
   "metadata": {},
   "source": [
    " #### Question-2. Count the number of taxi trips for each hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49a9c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup datetime string to timestamp\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp\n",
    "\n",
    "df = df.withColumn(\"pickup_datetime\", from_unixtime(unix_timestamp(\"tpep_pickup_datetime\", 'dd-MM-yyyy HH:mm')).cast(\"timestamp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60ab7c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|Hour|Number of Trips|\n",
      "+----+---------------+\n",
      "|   0|           8122|\n",
      "|   1|           6643|\n",
      "|   2|           5111|\n",
      "|   3|           5124|\n",
      "|   4|           7136|\n",
      "|   5|           6955|\n",
      "|   6|          14907|\n",
      "|   7|          19957|\n",
      "|   8|          24824|\n",
      "|   9|          28408|\n",
      "|  10|          31948|\n",
      "|  11|          35190|\n",
      "|  12|          38083|\n",
      "|  13|          39475|\n",
      "|  14|          40525|\n",
      "|  15|          40971|\n",
      "|  16|          38627|\n",
      "|  17|          38225|\n",
      "|  18|          34181|\n",
      "|  19|          26477|\n",
      "|  20|          18518|\n",
      "|  21|          15020|\n",
      "|  22|          13238|\n",
      "|  23|          12095|\n",
      "+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To count the number of taxi trips for each hour, we need to extract the hour from the tpep_pickup_datetime column and then group by this hour. Use the hour function to extract the hour from the tpep_pickup_datetime column, and then group by this hour and count.\n",
    "hourly_counts = df.groupBy(hour(\"pickup_datetime\").alias(\"Hour\")).agg(count(\"*\").alias(\"Number of Trips\")).orderBy(\"Hour\")\n",
    "\n",
    "# Show the hourly counts\n",
    "hourly_counts.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63279a72",
   "metadata": {},
   "source": [
    "### Question-2.2.  Create a table view of the data frame created in step 1 above and write SparkSQL queries to find out the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87b1c9",
   "metadata": {},
   "source": [
    " #### Question no.03. Average fare amount collected by hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82bd0dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|hour_of_day|      average_fare|\n",
      "+-----------+------------------+\n",
      "|          0|18.880695641467593|\n",
      "|          1|27.534966129760434|\n",
      "|          2| 30.12912737233388|\n",
      "|          3|  35.2012607338016|\n",
      "|          4|40.932777466367995|\n",
      "|          5|20.005923795830288|\n",
      "|          6| 11.69149459985242|\n",
      "|          7|11.342811043744058|\n",
      "|          8|11.184160087012579|\n",
      "|          9|11.276688256829068|\n",
      "|         10|11.909430324276952|\n",
      "|         11|11.991532821824373|\n",
      "|         12|11.864507260457405|\n",
      "|         13| 11.58097226092462|\n",
      "|         14|12.048847378161607|\n",
      "|         15|12.755428717873595|\n",
      "|         16|12.926983198280979|\n",
      "|         17|13.052417266187044|\n",
      "|         18|12.484358269213894|\n",
      "|         19|12.241942818295133|\n",
      "|         20| 13.67892590992549|\n",
      "|         21| 14.87007723035953|\n",
      "|         22|16.231789545248535|\n",
      "|         23|17.937012815212864|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, hour\n",
    "\n",
    "# Convert the string column to a timestamp column\n",
    "df_timestamp = df.withColumn(\"pickup_timestamp\", from_unixtime(unix_timestamp(\"tpep_pickup_datetime\", \"dd-MM-yyyy HH:mm\")))\n",
    "\n",
    "# Extract the hour from the new timestamp column\n",
    "df_with_hour = df_timestamp.withColumn(\"hour_of_day\", hour(\"pickup_timestamp\"))\n",
    "\n",
    "# Create a new temp view with the extracted hour\n",
    "df_with_hour.createOrReplaceTempView(\"taxi_data_with_hour\")\n",
    "\n",
    "# Now compute the average fare amount by hour of the day using SparkSQL\n",
    "avg_fare_by_hour = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        hour_of_day, \n",
    "        AVG(fare_amount) as average_fare\n",
    "    FROM \n",
    "        taxi_data_with_hour\n",
    "    WHERE\n",
    "        hour_of_day IS NOT NULL\n",
    "    GROUP BY \n",
    "        hour_of_day\n",
    "    ORDER BY \n",
    "        hour_of_day\n",
    "\"\"\")\n",
    "\n",
    "avg_fare_by_hour.show(24)  # to display averages for all 24 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcef1ce",
   "metadata": {},
   "source": [
    "#### Question-4. Average fare amount compared to the average trip distance.\n",
    "We want to calculate the average fare amount and average trip distance, and then determine the ratio of the average fare to the average distance for each hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f61a2a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+----------------------+\n",
      "|hour_of_day|      average_fare|  average_distance|fare_to_distance_ratio|\n",
      "+-----------+------------------+------------------+----------------------+\n",
      "|          0|18.880695641467593|5.3101957645899995|      3.55555547826877|\n",
      "|          1|27.534966129760434| 6.998961312659949|     3.934150354561082|\n",
      "|          2| 30.12912737233388| 7.810856975151618|    3.8573395298598507|\n",
      "|          3|  35.2012607338016| 28.23991608118659|    1.2465072712185803|\n",
      "|          4|40.932777466367995|11.320184977578476|    3.6159106540610617|\n",
      "|          5|20.005923795830288| 5.961084112149523|       3.3560881576986|\n",
      "|          6| 11.69149459985242| 3.082606829006503|     3.792729740892868|\n",
      "|          7|11.342811043744058|3.5050804229092534|    3.2361057879320803|\n",
      "|          8|11.184160087012579|2.5761774895262635|     4.341377926203854|\n",
      "|          9|11.276688256829068|2.5608205435088713|     4.403544904938007|\n",
      "|         10|11.909430324276952| 2.781062038312256|     4.282331771176321|\n",
      "|         11|11.991532821824373| 2.738920716112535|     4.378196400969379|\n",
      "|         12|11.864507260457405| 8.445872173935877|    1.4047699297500027|\n",
      "|         13| 11.58097226092462|2.5870150728309036|     4.476577033720898|\n",
      "|         14|12.048847378161607| 4.129674768661324|    2.9176262183153376|\n",
      "|         15|12.755428717873595|2.9969097654438523|     4.256193784995216|\n",
      "|         16|12.926983198280979| 3.640493437233023|    3.5508876533248754|\n",
      "|         17|13.052417266187044|3.1995482014388466|     4.079456362094289|\n",
      "|         18|12.484358269213894|3.1398695181533567|    3.9760755015566023|\n",
      "|         19|12.241942818295133| 3.167968425425843|    3.8642881412713423|\n",
      "|         20| 13.67892590992549|3.6439890916945665|    3.7538328369595555|\n",
      "|         21| 14.87007723035953|4.0594121171770965|    3.6631110124143147|\n",
      "|         22|16.231789545248535| 4.657468650853604|     3.485109780024742|\n",
      "|         23|17.937012815212864| 5.283533691608104|    3.3948894550823856|\n",
      "+-----------+------------------+------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the average fare amount and average trip distance by hour of the day using SparkSQL\n",
    "avg_fare_and_distance_by_hour = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        hour_of_day, \n",
    "        AVG(fare_amount) as average_fare,\n",
    "        AVG(trip_distance) as average_distance,\n",
    "        CASE \n",
    "            WHEN AVG(trip_distance) != 0 THEN AVG(fare_amount) / AVG(trip_distance)\n",
    "            ELSE NULL\n",
    "        END as fare_to_distance_ratio\n",
    "    FROM \n",
    "        taxi_data_with_hour\n",
    "    WHERE\n",
    "        hour_of_day IS NOT NULL\n",
    "    GROUP BY \n",
    "        hour_of_day\n",
    "    ORDER BY \n",
    "        hour_of_day\n",
    "\"\"\")\n",
    "\n",
    "avg_fare_and_distance_by_hour.show(24)  # to display averages for all 24 hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e27d85",
   "metadata": {},
   "source": [
    "#### Question no.5. Average fare amount and average trip distance by day of the week\n",
    "\n",
    "To calculate the average fare amount and average trip distance by day of the week, we would need to extract the day of the week from the tpep_pickup_datetime (or pickup_timestamp we've converted it to a timestamp format).\n",
    "\n",
    "Convert tpep_pickup_datetime to a timestamp format.<br>\n",
    "Extract the day of the week from the timestamp.<br>\n",
    "Group by the day of the week to compute the average fare and trip distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4220b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+\n",
      "|day_of_week|      average_fare|  average_distance|\n",
      "+-----------+------------------+------------------+\n",
      "|     Sunday|14.686292601998977| 3.910443634278223|\n",
      "|     Monday|13.409943486081076| 4.262469750235043|\n",
      "|    Tuesday| 13.44987940367479| 3.202572362689536|\n",
      "|  Wednesday|13.279588696533812|6.0476238081182645|\n",
      "|   Thursday|13.404982738924733|3.9995484502692804|\n",
      "|     Friday|13.702733958318719| 3.778188952327255|\n",
      "|   Saturday|13.880926298071149|3.6022067638824393|\n",
      "+-----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek, date_format\n",
    "\n",
    "# Convert the string column to a timestamp column \n",
    "df_timestamp = df.withColumn(\"pickup_timestamp\", from_unixtime(unix_timestamp(\"tpep_pickup_datetime\", \"dd-MM-yyyy HH:mm\")))\n",
    "\n",
    "# Extract the day of the week from the new timestamp column\n",
    "# we can use date_format to get the name of the day, or dayofweek to get the numeric representation (1 = Sunday, 2 = Monday, etc.)\n",
    "df_with_day = df_timestamp.withColumn(\"day_of_week\", date_format(\"pickup_timestamp\", \"EEEE\"))\n",
    "\n",
    "# Create a new temp view with the day of the week\n",
    "df_with_day.createOrReplaceTempView(\"taxi_data_with_day\")\n",
    "\n",
    "# Now compute the average fare amount and average trip distance by day of the week using SparkSQL\n",
    "avg_fare_and_distance_by_day = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        day_of_week, \n",
    "        AVG(fare_amount) as average_fare,\n",
    "        AVG(trip_distance) as average_distance\n",
    "    FROM \n",
    "        taxi_data_with_day\n",
    "    GROUP BY \n",
    "        day_of_week\n",
    "    ORDER BY \n",
    "        CASE day_of_week \n",
    "            WHEN 'Sunday' THEN 1\n",
    "            WHEN 'Monday' THEN 2\n",
    "            WHEN 'Tuesday' THEN 3\n",
    "            WHEN 'Wednesday' THEN 4\n",
    "            WHEN 'Thursday' THEN 5\n",
    "            WHEN 'Friday' THEN 6\n",
    "            WHEN 'Saturday' THEN 7\n",
    "        END\n",
    "\"\"\")\n",
    "\n",
    "avg_fare_and_distance_by_day.show(7)  # to display averages for all days of the week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a1148",
   "metadata": {},
   "source": [
    "#### Q 6 In the month of June 2020, find the zone which had maximum number of pick ups.\n",
    "\n",
    "To solve this, we need the PULocationID from the yellow_tripdata_2020-06.csv file to determine the pick-up locations and corresponding zone from  taxi+_zone_lookup.csv data which need to be read from HDFS into another dataframe.\n",
    "Join this new dataframe with the previously created dataframe on the PULocationID column ( PULocationID in yellow_tripdata_2020-06.csv matches LocationID in taxi+_zone_lookup.csv).<br>\n",
    "Group by the zone column and count the occurrences for each zone.<br>\n",
    "Order the result in descending order and pick the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30a0c318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "The zone with the maximum number of pickups in June 2020 is: Upper East Side North with 23098 pickups.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read the taxi+_zone_lookup.csv data\n",
    "zone_lookup_path = \"user/datat/input/taxi+_zone_lookup.csv\"\n",
    "zone_df = spark.read.csv(zone_lookup_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the schema to confirm the structure\n",
    "zone_df.printSchema()\n",
    "\n",
    "# Step 2: Join the two dataframes\n",
    "joined_df = df.join(zone_df, df.PULocationID == zone_df.LocationID, how='left')\n",
    "\n",
    "# Step 3: Group by zone and count\n",
    "zone_counts = joined_df.groupBy(\"Zone\").count()\n",
    "\n",
    "# Step 4: Order by count in descending order and display the first row\n",
    "top_zone = zone_counts.orderBy(\"count\", ascending=False).first()\n",
    "\n",
    "print(f\"The zone with the maximum number of pickups in June 2020 is: {top_zone['Zone']} with {top_zone['count']} pickups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3fa08",
   "metadata": {},
   "source": [
    "#### Q.07. In the month of June 2020, find the zone which had maximum number of drops. \n",
    "To find the zone with the maximum number of drops for June 2020, we would follow a similar approach to the previous answer. However, this time we would be focusing on the DOLocationID column (DOLocationID represents the drop-off location in the yellow_tripdata_2020-06.csv dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c8ccfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zone with the maximum number of drop-offs in June 2020 is: Upper East Side North with 22254 drop-offs.\n"
     ]
    }
   ],
   "source": [
    "# The zone lookup data is already loaded in the previous step as zone_df\n",
    "\n",
    "# Join the dataframe using DOLocationID for drop-offs\n",
    "joined_drop_df = df.join(zone_df, df.DOLocationID == zone_df.LocationID, how='left')\n",
    "\n",
    "# Group by Zone and count for drop-offs\n",
    "drop_zone_counts = joined_drop_df.groupBy(\"Zone\").count()\n",
    "\n",
    "# Order by count in descending order and get the top zone\n",
    "top_drop_zone = drop_zone_counts.orderBy(\"count\", ascending=False).first()\n",
    "\n",
    "print(f\"The zone with the maximum number of drop-offs in June 2020 is: {top_drop_zone['Zone']} with {top_drop_zone['count']} drop-offs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528283a",
   "metadata": {},
   "source": [
    "#### Q.08.Average no of passengers by hour of the day\n",
    "To compute the average number of passengers by hour of the day, we'll use the df_with_hour dataframe from our previously provided code, which already has the hour_of_day extracted from the pickup_timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60e6014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|hour_of_day|avg_passenger_count|\n",
      "+-----------+-------------------+\n",
      "|          0| 1.3300081766148815|\n",
      "|          1| 1.2963541666666667|\n",
      "|          2|  1.308466051969824|\n",
      "|          3| 1.3093270365997638|\n",
      "|          4| 1.2861205915813425|\n",
      "|          5| 1.3718697829716193|\n",
      "|          6| 1.3303137428192664|\n",
      "|          7| 1.3431017976810977|\n",
      "|          8|  1.359296915388592|\n",
      "|          9| 1.3521955975550306|\n",
      "|         10|  1.361447777998543|\n",
      "|         11| 1.3555843529624496|\n",
      "|         12| 1.3567879870492352|\n",
      "|         13| 1.3521634939012896|\n",
      "|         14| 1.3638007863695938|\n",
      "|         15| 1.3572915863345416|\n",
      "|         16|   1.35842077865147|\n",
      "|         17| 1.3650200560470356|\n",
      "|         18|  1.376865328634901|\n",
      "|         19| 1.3628078030060762|\n",
      "|         20| 1.3558048103607772|\n",
      "|         21| 1.3686376434914447|\n",
      "|         22| 1.3518518518518519|\n",
      "|         23| 1.3064166486017776|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by hour_of_day and compute average passengers\n",
    "avg_passengers_per_hour = df_with_hour.groupBy(\"hour_of_day\") \\\n",
    "                                     .agg(F.avg(\"passenger_count\").alias(\"avg_passenger_count\"))\n",
    "\n",
    "# Order the results by hour_of_day for better readability\n",
    "ordered_avg_passengers = avg_passengers_per_hour.orderBy(\"hour_of_day\")\n",
    "\n",
    "# Display the results\n",
    "ordered_avg_passengers.show(24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc077b8",
   "metadata": {},
   "source": [
    "#### Q.09. Total number of payments made by different type for the month.\n",
    "To determine the total number of payments made by different payment types for the month, we would group by month and payment_type column and count the number of occurrences for each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a372feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+------------------+\n",
      "|Month|payment_type|TotalCount|PaymentDescription|\n",
      "+-----+------------+----------+------------------+\n",
      "|    1|           2|         3|              Cash|\n",
      "|    5|           1|         1|       Credit card|\n",
      "|    5|           2|         3|              Cash|\n",
      "|    6|        null|     50717|             Other|\n",
      "|    6|           1|    322565|       Credit card|\n",
      "|    6|           2|    168937|              Cash|\n",
      "|    6|           3|      5245|         No charge|\n",
      "|    6|           4|      2275|           Dispute|\n",
      "|    6|           5|        12|           Unknown|\n",
      "|    7|           1|         2|       Credit card|\n",
      "+-----+------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convert the string column to a timestamp column\n",
    "df_timestamp = df.withColumn(\"pickup_timestamp\", F.from_unixtime(F.unix_timestamp(\"tpep_pickup_datetime\", \"dd-MM-yyyy HH:mm\")))\n",
    "\n",
    "# Extract the month from the timestamp column\n",
    "df_month = df_timestamp.withColumn(\"Month\", F.month(\"pickup_timestamp\"))\n",
    "\n",
    "# Generate the PaymentDescription column\n",
    "payment_description = F.when(df_month[\"payment_type\"] == '1', 'Credit card') \\\n",
    "                       .when(df_month[\"payment_type\"] == '2', 'Cash') \\\n",
    "                       .when(df_month[\"payment_type\"] == '3', 'No charge') \\\n",
    "                       .when(df_month[\"payment_type\"] == '4', 'Dispute') \\\n",
    "                       .when(df_month[\"payment_type\"] == '5', 'Unknown') \\\n",
    "                       .when(df_month[\"payment_type\"] == '6', 'Voided trip') \\\n",
    "                       .otherwise('Other').alias('PaymentDescription')\n",
    "\n",
    "# Group by month and payment_type, then aggregate\n",
    "agg_df = df_month.groupBy(\"Month\", \"payment_type\") \\\n",
    "                 .agg(F.count(\"*\").alias(\"TotalCount\"))\n",
    "\n",
    "# Add the PaymentDescription column to the result\n",
    "result_df = agg_df.withColumn(\"PaymentDescription\", payment_description)\n",
    "\n",
    "# Order the result by Month and payment_type\n",
    "ordered_result = result_df.orderBy(\"Month\", \"payment_type\")\n",
    "\n",
    "ordered_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c6f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
